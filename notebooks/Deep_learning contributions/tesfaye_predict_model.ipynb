{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile\n",
    "import json\n",
    "\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "import _pickle as pickle\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_sequence_to_text(int_sequence):\n",
    "    \"\"\" Convert an integer sequence to text \"\"\"\n",
    "    text = []\n",
    "    for c in int_sequence:\n",
    "        ch = index_map[c]\n",
    "        text.append(ch)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_gen, index, partition, model, verbose=True):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        data_gen: Data to run prediction on\n",
    "        index (int): Example to visualize\n",
    "        partition (str): Either 'train' or 'validation'\n",
    "        model (Model): The acoustic model\n",
    "    \"\"\"\n",
    "    audio_path,data_point,transcr,prediction = predict_raw(data_gen, index, partition, model)\n",
    "    output_length = [model.output_length(data_point.shape[0])]\n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length, greedy=False)[0][0])+1).flatten().tolist()\n",
    "    predicted = ''.join(int_sequence_to_text(pred_ints)).replace(\"<SPACE>\", \" \")\n",
    "    wer_val = wer(transcr, predicted)\n",
    "    if verbose:\n",
    "        display(Audio(audio_path, embed=True))\n",
    "        print('Truth: ' + transcr)\n",
    "        print('Predicted: ' + predicted)\n",
    "        print(\"wer: %d\" % wer_val)\n",
    "    return wer_val\n",
    "def predict_raw(data_gen, index, partition, model):\n",
    "    \"\"\" Get a model's decoded predictions\n",
    "    Params:\n",
    "        data_gen: Data to run prediction on\n",
    "        index (int): Example to visualize\n",
    "        partition (str): Either 'train' or 'validation'\n",
    "        model (Model): The acoustic model\n",
    "    \"\"\"\n",
    "\n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    prediction = model.predict(np.expand_dims(data_point, axis=0))\n",
    "    return (audio_path,data_point,transcr,prediction)\n",
    "\n",
    "\n",
    "def calculate_wer(model, model_name, data_gen, partition, length):\n",
    "    start = time.time()\n",
    "    def wer_single(i):\n",
    "        wer = predict(data_gen, i, partition, model, verbose=False)\n",
    "        if (i%100==0) and i>0:\n",
    "            print(\"processed %d in %d minutes\" % (i, ((time.time() - start)/60)))\n",
    "        return wer\n",
    "    wer = list(map(lambda i: wer_single(i), range(1, length)))\n",
    "    print(\"Total time: %f minutes\" % ((time.time() - start)/60))\n",
    "    filename = 'model/' + model_name + '_' + partition + '_wer.pickle'\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(wer, handle)\n",
    "    return wer\n",
    "\n",
    "\n",
    "\n",
    "def load_wer(model_name, partition):\n",
    "    filename = 'model/' + model_name + '_' + partition + '_wer.pickle'\n",
    "    return pickle.load(open(filename, \"rb\"))\n",
    "def plot_raw_audio(vis_raw_audio, title='Audio Signal', size=(12, 3)):\n",
    "    fig = plt.figure(figsize=size)\n",
    "    ax = fig.add_subplot(111)\n",
    "    steps = len(vis_raw_audio)\n",
    "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mfcc_feature(vis_mfcc_feature):\n",
    "    # plot the MFCC feature\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
    "    # plot the normalized spectrogram\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized Spectrogram')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('Frequency')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_raw(name):\n",
    "    raw_audio = librosa.load(\"test/\" + name)[0]\n",
    "    plot_raw_audio(raw_audio, size=(12,2))\n",
    "\n",
    "def plot_spect(name):\n",
    "    plot_spectrogram_feature(spectrogram_from_file(\"test/\" + name))\n",
    "\n",
    "def plot_mfcc(name):\n",
    "    (rate, sig) = wav.read(\"test/\" + name)\n",
    "    plot_mfcc_feature(mfcc(sig, rate, numcep=13))\n",
    "\n",
    "plot_mfcc(\"speaker1_4.wav\")\n",
    "plot_mfcc(\"speaker1_2.wav\")\n",
    "\n",
    "\n",
    "plot_spect(\"speaker1_4.wav\")\n",
    "plot_spect(\"speaker1_2.wav\")\n",
    "\n",
    "\n",
    "plot_raw(\"speaker1_4.wav\")\n",
    "plot_raw(\"speaker1_2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported = \"\"\"\n",
    "ሀ ሁ ሂ ሄ ህ ሆ\n",
    "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "ኋ\n",
    "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "አ ኡ ኢ ኤ እ ኦ\n",
    "ኧ\n",
    "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
    "ኳ\n",
    "ጾ\n",
    "ጻ\n",
    "ኸ\n",
    "ኲ\n",
    "ጽ\n",
    "ጸ\n",
    "ጹ\n",
    "ጺ\n",
    "ጼ\n",
    "ኰ\n",
    "ጿ\n",
    "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
    "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "\"\"\".split()\n",
    "\n",
    "char_map = {}\n",
    "char_map[\"\"] = 0\n",
    "char_map[\"<SPACE>\"] = 1\n",
    "index = 2\n",
    "for c in supported:\n",
    "    char_map[c] = index\n",
    "    index += 1\n",
    "index_map = {v+1: k for k, v in char_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 13)]        0         \n",
      "                                                                 \n",
      " rnn (GRU)                   (None, None, 5)           300       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, None, 5)          20        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, None, 234)        1404      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 234)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,724\n",
      "Trainable params: 1,714\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "def model_1(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units, activation=activation,\n",
    "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='../model/model_1.png')\n",
    "    return model\n",
    "print(len(char_map))\n",
    "model = model_1(input_dim=13,\n",
    "                units=5,\n",
    "                activation='relu',\n",
    "                output_dim=len(char_map)+1)\n",
    "model.load_weights('../model/RNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map[0]=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\10AcademyProjects\\speech-to-text\\Speech-to-Text\\notebooks\\tesfaye_predict_model.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/10AcademyProjects/speech-to-text/Speech-to-Text/notebooks/tesfaye_predict_model.ipynb#ch0000009?line=0'>1</a>\u001b[0m predict(audio_gen,\u001b[39m14\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/10AcademyProjects/speech-to-text/Speech-to-Text/notebooks/tesfaye_predict_model.ipynb#ch0000009?line=1'>2</a>\u001b[0m _,_,_,raw_pred \u001b[39m=\u001b[39m predict_raw(audio_gen,\u001b[39m14\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/10AcademyProjects/speech-to-text/Speech-to-Text/notebooks/tesfaye_predict_model.ipynb#ch0000009?line=2'>3</a>\u001b[0m raw_pred_char \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([\u001b[39msorted\u001b[39m(char_map\u001b[39m.\u001b[39mkeys(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m k: char_map[k]) \u001b[39m+\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mBLANK\u001b[39m\u001b[39m'\u001b[39m], raw_pred[\u001b[39m0\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'audio_gen' is not defined"
     ]
    }
   ],
   "source": [
    "predict(audio_gen,14, 'train', model)\n",
    "_,_,_,raw_pred = predict_raw(audio_gen,14, 'train', model)\n",
    "raw_pred_char = np.vstack([sorted(char_map.keys(), key=lambda k: char_map[k]) + ['BLANK'], raw_pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
