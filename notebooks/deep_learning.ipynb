{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 18:05:43.080531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:05:43.080563: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "import keras\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras import backend as K\n",
    "from keras import regularizers, callbacks\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Flatten, Embedding, Activation, GRUCell, LSTMCell,SimpleRNNCell\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution1D, Conv1D, SimpleRNN, GRU, LSTM, CuDNNLSTM, CuDNNGRU, Conv2D\n",
    "from keras.layers import LeakyReLU, PReLU, ThresholdedReLU, ELU\n",
    "from keras.layers import BatchNormalization, TimeDistributed, Bidirectional\n",
    "from keras.layers import Wrapper\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.utils import np_utils\n",
    "from keras import constraints, initializers, regularizers\n",
    "import keras.losses\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_rnn_model(input_dim, output_dim=29):\n",
    "    # Input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Recurrent layer\n",
    "    simp_rnn = GRU(output_dim, return_sequences=True, \n",
    "                 implementation=2, name='rnn')(input_data)\n",
    "    # Softmax Activation Layer\n",
    "    y_pred = Activation('softmax', name='softmax')(simp_rnn)\n",
    "    # Specifying the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 161)]       0         \n",
      "                                                                 \n",
      " rnn (GRU)                   (None, None, 29)          16704     \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 29)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,704\n",
      "Trainable params: 16,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 18:06:51.132541: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-07 18:06:51.132583: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-60-46.ec2.internal): /proc/driver/nvidia/version does not exist\n",
      "2022-06-07 18:06:51.132991: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_0 = regular_rnn_model(input_dim=161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "from prep import prep\n",
    "from AudioGenerator import AudioGenerator\n",
    "audio_gen = AudioGenerator(spectrogram=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py:483: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "583/583 [==============================] - 149s 251ms/step - loss: 1748.3467 - val_loss: 1593.5072 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "583/583 [==============================] - 148s 255ms/step - loss: 1044.2191 - val_loss: 833.3390 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "583/583 [==============================] - 149s 255ms/step - loss: 888.0744 - val_loss: 815.0851 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "583/583 [==============================] - 148s 254ms/step - loss: 885.6637 - val_loss: 812.7409 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "583/583 [==============================] - 149s 255ms/step - loss: 885.4000 - val_loss: 814.3174 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "583/583 [==============================] - 149s 256ms/step - loss: 885.1489 - val_loss: 814.3876 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "audio_gen.train_model(input_to_softmax=model_0, \n",
    "            pickle_path='model_0.pickle', \n",
    "            save_model_path='model_0.h5',\n",
    "            spectrogram=True,\n",
    "           ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brnn_tdd_model(input_dim, units, activation, output_dim=29):\n",
    "    # Input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Bidirectional recurrent layer\n",
    "    brnn = Bidirectional(LSTM(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, name='brnn'))(input_data)\n",
    "    # TimeDistributed Dense layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(brnn)\n",
    "    # Softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specifying the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 161)]       0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 400)        579200    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 29)         11629     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 29)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 590,829\n",
      "Trainable params: 590,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = brnn_tdd_model(input_dim=161, units=200, activation='relu') # 161 for Spectrogram/13 for MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py:483: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/583 [..............................] - ETA: 23:58 - loss: 6514.1245Batch 1: Invalid loss, terminating training\n",
      "583/583 [==============================] - 29s 45ms/step - loss: nan - val_loss: nan - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "audio_gen.train_model(input_to_softmax=model_2, \n",
    "            pickle_path='model_2.pickle', \n",
    "            save_model_path='model_2.h5', \n",
    "            spectrogram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + Deeper Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_deep_brnn_tdd_model(input_dim, filters, activation, kernel_size, conv_stride,\n",
    "    conv_border_mode, recur_layers, units, output_dim=29):\n",
    "    # Input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Convolutional layer\n",
    "    conv_1d = Conv1D(filters, kernel_size, \n",
    "                     strides=conv_stride, \n",
    "                     padding=conv_border_mode,\n",
    "                     activation=activation,\n",
    "                     name='conv1d')(input_data)\n",
    "    # Batch normalization\n",
    "    bn_cnn = BatchNormalization()(conv_1d)\n",
    "    # Bidirectional recurrent layer\n",
    "    brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, name='brnn'))(bn_cnn)\n",
    "    # Batch normalization \n",
    "    bn_rnn = BatchNormalization()(brnn)\n",
    "    # Loop for additional layers\n",
    "    for i in range(recur_layers - 1):\n",
    "        name = 'brnn_' + str(i + 1)\n",
    "        brnn = Bidirectional(GRU(units, activation=activation, \n",
    "        return_sequences=True, implementation=2, name=name))(bn_rnn)\n",
    "        bn_rnn = BatchNormalization()(brnn)\n",
    "    # TimeDistributed Dense layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specifying the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: audio_gen.cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " the_input (InputLayer)      [(None, None, 161)]       0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 200)         354400    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, 200)        800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 400)        482400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, None, 400)        1600      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 400)        722400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, None, 400)        1600      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 29)         11629     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " softmax (Activation)        (None, None, 29)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,574,829\n",
      "Trainable params: 1,572,829\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = cnn_deep_brnn_tdd_model(input_dim=161, # 161 for Spectrogram/13 for MFCC\n",
    "                                  filters=200,\n",
    "                                  activation='relu',\n",
    "                                  kernel_size=11, \n",
    "                                  conv_stride=2,\n",
    "                                  conv_border_mode='valid',\n",
    "                                  recur_layers=2,\n",
    "                                  units=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py:483: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583/583 [==============================] - 522s 888ms/step - loss: 3.2970 - val_loss: -0.6921 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "583/583 [==============================] - 532s 913ms/step - loss: -0.6918 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "583/583 [==============================] - 532s 912ms/step - loss: -0.6923 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "583/583 [==============================] - 531s 911ms/step - loss: -0.6923 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "583/583 [==============================] - 529s 907ms/step - loss: -0.6923 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "583/583 [==============================] - 529s 908ms/step - loss: -0.6923 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "583/583 [==============================] - 531s 910ms/step - loss: -0.6923 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "583/583 [==============================] - 532s 913ms/step - loss: -0.6924 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "583/583 [==============================] - 531s 911ms/step - loss: -0.6924 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "583/583 [==============================] - 530s 910ms/step - loss: -0.6924 - val_loss: -0.6924 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      " 67/583 [==>...........................] - ETA: 7:37 - loss: -0.6924"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/abel_mitiku/root/Speech-to-Text/notebooks/deep_learning.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bg2/home/abel_mitiku/root/Speech-to-Text/notebooks/deep_learning.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m audio_gen\u001b[39m.\u001b[39;49mtrain_model(input_to_softmax\u001b[39m=\u001b[39;49mmodel_3, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bg2/home/abel_mitiku/root/Speech-to-Text/notebooks/deep_learning.ipynb#ch0000024vscode-remote?line=1'>2</a>\u001b[0m             pickle_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel_3.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bg2/home/abel_mitiku/root/Speech-to-Text/notebooks/deep_learning.ipynb#ch0000024vscode-remote?line=2'>3</a>\u001b[0m             save_model_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel_3.h5\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bg2/home/abel_mitiku/root/Speech-to-Text/notebooks/deep_learning.ipynb#ch0000024vscode-remote?line=3'>4</a>\u001b[0m             spectrogram\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py:483\u001b[0m, in \u001b[0;36mAudioGenerator.train_model\u001b[0;34m(self, input_to_softmax, pickle_path, save_model_path, train_json, valid_json, minibatch_size, spectrogram, mfcc_dim, optimizer, epochs, verbose, sort_by_duration, max_duration)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=474'>475</a>\u001b[0m tensor_boarder \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mTensorBoard(\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=475'>476</a>\u001b[0m     log_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./logs\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=476'>477</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=479'>480</a>\u001b[0m     write_images\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=480'>481</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=481'>482</a>\u001b[0m \u001b[39m# Fit/train model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=482'>483</a>\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit_generator(\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=483'>484</a>\u001b[0m     generator\u001b[39m=\u001b[39;49maudio_gen\u001b[39m.\u001b[39;49mnext_train(),\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=484'>485</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=485'>486</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=486'>487</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49maudio_gen\u001b[39m.\u001b[39;49mnext_valid(),\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=487'>488</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=488'>489</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=489'>490</a>\u001b[0m         checkpointer,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=490'>491</a>\u001b[0m         terminator,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=491'>492</a>\u001b[0m         logger,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=492'>493</a>\u001b[0m         time_machiner,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=493'>494</a>\u001b[0m         tensor_boarder,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=494'>495</a>\u001b[0m         stopper,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=495'>496</a>\u001b[0m         reducer,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=496'>497</a>\u001b[0m     ],\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=497'>498</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=498'>499</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=499'>500</a>\u001b[0m \u001b[39m# Save model loss\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/abel_mitiku/root/Speech-to-Text/notebooks/../scripts/AudioGenerator.py?line=500'>501</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../model/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m pickle_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py:2260\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2248'>2249</a>\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2249'>2250</a>\u001b[0m \n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2250'>2251</a>\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2251'>2252</a>\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2252'>2253</a>\u001b[0m \u001b[39m  this endpoint.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2253'>2254</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2254'>2255</a>\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2255'>2256</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2256'>2257</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2257'>2258</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2258'>2259</a>\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m-> <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2259'>2260</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2260'>2261</a>\u001b[0m     generator,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2261'>2262</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2262'>2263</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2263'>2264</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2264'>2265</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2265'>2266</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2266'>2267</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2267'>2268</a>\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2268'>2269</a>\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2269'>2270</a>\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2270'>2271</a>\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2271'>2272</a>\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2272'>2273</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=2273'>2274</a>\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1401'>1402</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1402'>1403</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1403'>1404</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1404'>1405</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1405'>1406</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1406'>1407</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1407'>1408</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1408'>1409</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1409'>1410</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/keras/engine/training.py?line=1410'>1411</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2449'>2450</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2450'>2451</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2451'>2452</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2452'>2453</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=2453'>2454</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1859'>1860</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1860'>1861</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1861'>1862</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1862'>1863</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1863'>1864</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1864'>1865</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=1865'>1866</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=494'>495</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=495'>496</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=508'>509</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/function.py?line=509'>510</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///opt/miniconda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_gen.train_model(input_to_softmax=model_3, \n",
    "            pickle_path='model_3.pickle', \n",
    "            save_model_path='model_3.h5', \n",
    "            spectrogram=True) # True for Spectrogram/False for MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c2b8563932708ac4f64e3bc4797ea8eb43fc7530baa7f01daadf5b46d7b1fd4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
