{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile\n",
    "import json\n",
    "\n",
    "import random\n",
    "from python_speech_features import mfcc\n",
    "import librosa\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "import _pickle as pickle\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported = \"\"\"\n",
    "ሀ ሁ ሂ ሄ ህ ሆ\n",
    "ለ ሉ ሊ ላ ሌ ል ሎ ሏ\n",
    "መ ሙ ሚ ማ ሜ ም ሞ ሟ\n",
    "ረ ሩ ሪ ራ ሬ ር ሮ ሯ\n",
    "ሰ ሱ ሲ ሳ ሴ ስ ሶ ሷ\n",
    "ሸ ሹ ሺ ሻ ሼ ሽ ሾ ሿ\n",
    "ቀ ቁ ቂ ቃ ቄ ቅ ቆ ቋ\n",
    "በ ቡ ቢ ባ ቤ ብ ቦ ቧ\n",
    "ቨ ቩ ቪ ቫ ቬ ቭ ቮ ቯ\n",
    "ተ ቱ ቲ ታ ቴ ት ቶ ቷ\n",
    "ቸ ቹ ቺ ቻ ቼ ች ቾ ቿ\n",
    "ኋ\n",
    "ነ ኑ ኒ ና ኔ ን ኖ ኗ\n",
    "ኘ ኙ ኚ ኛ ኜ ኝ ኞ ኟ\n",
    "አ ኡ ኢ ኤ እ ኦ\n",
    "ኧ\n",
    "ከ ኩ ኪ ካ ኬ ክ ኮ\n",
    "ኳ\n",
    "ወ ዉ ዊ ዋ ዌ ው ዎ\n",
    "ዘ ዙ ዚ ዛ ዜ ዝ ዞ ዟ\n",
    "ዠ ዡ ዢ ዣ ዤ ዥ ዦ ዧ\n",
    "የ ዩ ዪ ያ ዬ ይ ዮ\n",
    "ደ ዱ ዲ ዳ ዴ ድ ዶ ዷ\n",
    "ጀ ጁ ጂ ጃ ጄ ጅ ጆ ጇ\n",
    "ገ ጉ ጊ ጋ ጌ ግ ጐ ጓ ጔ\n",
    "ጠ ጡ ጢ ጣ ጤ ጥ ጦ ጧ\n",
    "ጨ ጩ ጪ ጫ ጬ ጭ ጮ ጯ\n",
    "ጰ ጱ ጲ ጳ ጴ ጵ ጶ ጷ\n",
    "ፀ ፁ ፂ ፃ ፄ ፅ ፆ ፇ\n",
    "ፈ ፉ ፊ ፋ ፌ ፍ ፎ ፏ\n",
    "ፐ ፑ ፒ ፓ ፔ ፕ ፖ\n",
    "\"\"\".split()\n",
    "\n",
    "char_map = {}\n",
    "char_map[\"\"] = 0\n",
    "char_map[\"<SPACE>\"] = 1\n",
    "index = 2\n",
    "for c in supported:\n",
    "    char_map[c] = index\n",
    "    index += 1\n",
    "index_map = {v+1: k for k, v in char_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_feat_dim(window, max_freq):\n",
    "    return int(0.001 * window * max_freq) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n",
    "                          eps=1e-14):\n",
    "    \"\"\" Calculate the log of linear spectrogram from FFT energy\n",
    "    Params:\n",
    "        filename (str): Path to the audio file\n",
    "        step (int): Step size in milliseconds between windows\n",
    "        window (int): FFT window size in milliseconds\n",
    "        max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "            [0, max_freq] are returned\n",
    "        eps (float): Small value to ensure numerical stability (for ln(x))\n",
    "    \"\"\"\n",
    "    with soundfile.SoundFile(filename) as sound_file:\n",
    "        audio = sound_file.read(dtype='float32')\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if audio.ndim >= 2:\n",
    "            audio = np.mean(audio, 1)\n",
    "        if max_freq is None:\n",
    "            max_freq = sample_rate / 2\n",
    "        if max_freq > sample_rate / 2:\n",
    "            raise ValueError(\"max_freq must not be greater than half of \"\n",
    "                             \" sample rate\")\n",
    "        if step > window:\n",
    "            raise ValueError(\"step size must not be greater than window size\")\n",
    "        hop_length = int(0.001 * step * sample_rate)\n",
    "        fft_length = int(0.001 * window * sample_rate)\n",
    "        pxx, freqs = spectrogram(\n",
    "            audio, fft_length=fft_length, sample_rate=sample_rate,\n",
    "            hop_length=hop_length)\n",
    "        ind = np.where(freqs <= max_freq)[0][-1] + 1\n",
    "    return np.transpose(np.log(pxx[:ind, :] + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_int_sequence(text):\n",
    "    \"\"\" Convert text to an integer sequence \"\"\"\n",
    "    int_sequence = []\n",
    "    for c in text:\n",
    "        if c == ' ':\n",
    "            ch = char_map['<SPACE>']\n",
    "        else:\n",
    "            # print(\"checking character \" + c + \" in map:\")\n",
    "            # print(char_map)\n",
    "            ch = char_map[c]\n",
    "        int_sequence.append(ch)\n",
    "    return int_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_sequence_to_text(int_sequence):\n",
    "    \"\"\" Convert an integer sequence to text \"\"\"\n",
    "    text = []\n",
    "    for c in int_sequence:\n",
    "        ch = index_map[c]\n",
    "        text.append(ch)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://martin-thoma.com/word-error-rate-calculation/\n",
    "def wer(r, h):\n",
    "    \"\"\"\n",
    "    Calculation of WER with Levenshtein distance.\n",
    "\n",
    "    Works only for iterables up to 254 elements (uint8).\n",
    "    O(nm) time ans space complexity.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : list\n",
    "    h : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "    1\n",
    "    >>> wer(\"who is there\".split(), \"\".split())\n",
    "    3\n",
    "    >>> wer(\"\".split(), \"who is there\".split())\n",
    "    3\n",
    "    \"\"\"\n",
    "    # initialisation\n",
    "    import numpy\n",
    "    d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
    "    d = d.reshape((len(r)+1, len(h)+1))\n",
    "    for i in range(len(r)+1):\n",
    "        for j in range(len(h)+1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion    = d[i][j-1] + 1\n",
    "                deletion     = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    return d[len(r)][len(h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for plotting\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hist(p):\n",
    "    hist = pickle.load(open( \"models/\" + p + \".pickle\", \"rb\"))\n",
    "    plt.plot(hist['loss'], label=\"train\")\n",
    "    plt.plot(hist['val_loss'], label=\"valid\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RNG_SEED = 123\n",
    "\n",
    "def make_audio_gen(train_json,\n",
    "                   valid_json,\n",
    "                   minibatch_size=20,\n",
    "                   spectrogram=True,\n",
    "                   mfcc_dim=13,\n",
    "                   sort_by_duration=False,\n",
    "                   max_duration=10.0):\n",
    "    return AudioGenerator(train_json, valid_json, minibatch_size=minibatch_size, \n",
    "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
    "        sort_by_duration=sort_by_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines a class that is used to featurize audio clips, and provide\n",
    "them to the network for training or testing.\n",
    "\"\"\"\n",
    "class AudioGenerator():\n",
    "    def __init__(self, train_corpus, valid_corpus, step=10, window=20, max_freq=8000, mfcc_dim=13,\n",
    "        minibatch_size=20, desc_file=None, spectrogram=True, max_duration=10.0, \n",
    "        sort_by_duration=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            step (int): Step size in milliseconds between windows (for spectrogram ONLY)\n",
    "            window (int): FFT window size in milliseconds (for spectrogram ONLY)\n",
    "            max_freq (int): Only FFT bins corresponding to frequencies between\n",
    "                [0, max_freq] are returned (for spectrogram ONLY)\n",
    "            desc_file (str, optional): Path to a JSON-line file that contains\n",
    "                labels and paths to the audio files. If this is None, then\n",
    "                load metadata right away\n",
    "        \"\"\"\n",
    "        self.train_corpus = train_corpus\n",
    "        self.valid_corpus = valid_corpus\n",
    "        self.feat_dim = calc_feat_dim(window, max_freq)\n",
    "        self.mfcc_dim = mfcc_dim\n",
    "        self.feats_mean = np.zeros((self.feat_dim,))\n",
    "        self.feats_std = np.ones((self.feat_dim,))\n",
    "        self.rng = random.Random(RNG_SEED)\n",
    "        if desc_file is not None:\n",
    "            self.load_metadata_from_desc_file(desc_file)\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "        self.max_freq = max_freq\n",
    "        self.cur_train_index = 0\n",
    "        self.cur_valid_index = 0\n",
    "        self.cur_test_index = 0\n",
    "        self.max_duration=max_duration\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.sort_by_duration = sort_by_duration\n",
    "\n",
    "    def get_batch(self, partition):\n",
    "        \"\"\" Obtain a batch of train, validation, or test data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            audio_paths = self.train_audio_paths\n",
    "            cur_index = self.cur_train_index\n",
    "            texts = self.train_texts\n",
    "        elif partition == 'valid':\n",
    "            audio_paths = self.valid_audio_paths\n",
    "            cur_index = self.cur_valid_index\n",
    "            texts = self.valid_texts\n",
    "        elif partition == 'test':\n",
    "            audio_paths = self.test_audio_paths\n",
    "            cur_index = self.test_valid_index\n",
    "            texts = self.test_texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "        features = [self.normalize(self.featurize(a)) for a in \n",
    "            audio_paths[cur_index:cur_index+self.minibatch_size]]\n",
    "\n",
    "        # calculate necessary sizes\n",
    "        max_length = max([features[i].shape[0] \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        max_string_length = max([len(texts[cur_index+i]) \n",
    "            for i in range(0, self.minibatch_size)])\n",
    "        \n",
    "        # initialize the arrays\n",
    "        X_data = np.zeros([self.minibatch_size, max_length, \n",
    "            self.feat_dim*self.spectrogram + self.mfcc_dim*(not self.spectrogram)])\n",
    "        labels = np.ones([self.minibatch_size, max_string_length]) * 28\n",
    "        input_length = np.zeros([self.minibatch_size, 1])\n",
    "        label_length = np.zeros([self.minibatch_size, 1])\n",
    "        \n",
    "        for i in range(0, self.minibatch_size):\n",
    "            # calculate X_data & input_length\n",
    "            feat = features[i]\n",
    "            input_length[i] = feat.shape[0]\n",
    "            X_data[i, :feat.shape[0], :] = feat\n",
    "\n",
    "            # calculate labels & label_length\n",
    "            label = np.array(text_to_int_sequence(texts[cur_index+i])) \n",
    "            labels[i, :len(label)] = label\n",
    "            label_length[i] = len(label)\n",
    " \n",
    "        # return the arrays\n",
    "        outputs = {'ctc': np.zeros([self.minibatch_size])}\n",
    "        inputs = {'the_input': X_data, \n",
    "                  'the_labels': labels, \n",
    "                  'input_length': input_length, \n",
    "                  'label_length': label_length \n",
    "                 }\n",
    "        return (inputs, outputs)\n",
    "\n",
    "    def shuffle_data_by_partition(self, partition):\n",
    "        \"\"\" Shuffle the training or validation data\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = shuffle_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "            self.train_length = len(self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = shuffle_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "            self.valid_length = len(self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def sort_data_by_duration(self, partition):\n",
    "        \"\"\" Sort the training or validation sets by (increasing) duration\n",
    "        \"\"\"\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths, self.train_durations, self.train_texts = sort_data(\n",
    "                self.train_audio_paths, self.train_durations, self.train_texts)\n",
    "        elif partition == 'valid':\n",
    "            self.valid_audio_paths, self.valid_durations, self.valid_texts = sort_data(\n",
    "                self.valid_audio_paths, self.valid_durations, self.valid_texts)\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition. \"\n",
    "                \"Must be train/validation\")\n",
    "\n",
    "    def next_train(self):\n",
    "        \"\"\" Obtain a batch of training data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('train')\n",
    "            self.cur_train_index += self.minibatch_size\n",
    "            if self.cur_train_index >= len(self.train_texts) - self.minibatch_size:\n",
    "                self.cur_train_index = 0\n",
    "                self.shuffle_data_by_partition('train')\n",
    "            yield ret    \n",
    "\n",
    "    def next_valid(self):\n",
    "        \"\"\" Obtain a batch of validation data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('valid')\n",
    "            self.cur_valid_index += self.minibatch_size\n",
    "            if self.cur_valid_index >= len(self.valid_texts) - self.minibatch_size:\n",
    "                self.cur_valid_index = 0\n",
    "                self.shuffle_data_by_partition('valid')\n",
    "            yield ret\n",
    "\n",
    "    def next_test(self):\n",
    "        \"\"\" Obtain a batch of test data\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ret = self.get_batch('test')\n",
    "            self.cur_test_index += self.minibatch_size\n",
    "            if self.cur_test_index >= len(self.test_texts) - self.minibatch_size:\n",
    "                self.cur_test_index = 0\n",
    "            yield ret\n",
    "\n",
    "    def load_train_data(self):\n",
    "        desc_file=self.train_corpus\n",
    "        self.load_metadata_from_desc_file(desc_file, 'train')\n",
    "        self.fit_train()\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('train')\n",
    "\n",
    "    def load_validation_data(self):\n",
    "        desc_file=self.valid_corpus\n",
    "        self.load_metadata_from_desc_file(desc_file, 'validation')\n",
    "        if self.sort_by_duration:\n",
    "            self.sort_data_by_duration('valid')\n",
    "\n",
    "    def load_test_data(self):\n",
    "        desc_file='test_corpus.json'\n",
    "        self.load_metadata_from_desc_file(desc_file, 'test')\n",
    "    \n",
    "    def load_metadata_from_desc_file(self, desc_file, partition):\n",
    "        \"\"\" Read metadata from a JSON-line file\n",
    "            (possibly takes long, depending on the filesize)\n",
    "        Params:\n",
    "            desc_file (str):  Path to a JSON-line file that contains labels and\n",
    "                paths to the audio files\n",
    "            partition (str): One of 'train', 'validation' or 'test'\n",
    "        \"\"\"\n",
    "        audio_paths, durations, texts = [], [], []\n",
    "        with open(desc_file, encoding='utf-8') as json_line_file:\n",
    "            for line_num, json_line in enumerate(json_line_file):\n",
    "                try:\n",
    "                    spec = json.loads(json_line)\n",
    "                    if float(spec['duration']) > self.max_duration:\n",
    "                        continue\n",
    "                    audio_paths.append(spec['key'])\n",
    "                    durations.append(float(spec['duration']))\n",
    "                    texts.append(spec['text'])\n",
    "                except Exception as e:\n",
    "                    # Change to (KeyError, ValueError) or\n",
    "                    # (KeyError,json.decoder.JSONDecodeError), depending on\n",
    "                    # json module version\n",
    "                    print('Error reading line #{}: {}'\n",
    "                                .format(line_num, json_line))\n",
    "        if partition == 'train':\n",
    "            self.train_audio_paths = audio_paths\n",
    "            self.train_durations = durations\n",
    "            self.train_texts = texts\n",
    "        elif partition == 'validation':\n",
    "            self.valid_audio_paths = audio_paths\n",
    "            self.valid_durations = durations\n",
    "            self.valid_texts = texts\n",
    "        elif partition == 'test':\n",
    "            self.test_audio_paths = audio_paths\n",
    "            self.test_durations = durations\n",
    "            self.test_texts = texts\n",
    "        else:\n",
    "            raise Exception(\"Invalid partition to load metadata. \"\n",
    "             \"Must be train/validation/test\")\n",
    "            \n",
    "    def fit_train(self, k_samples=100):\n",
    "        \"\"\" Estimate the mean and std of the features from the training set\n",
    "        Params:\n",
    "            k_samples (int): Use this number of samples for estimation\n",
    "        \"\"\"\n",
    "        k_samples = min(k_samples, len(self.train_audio_paths))\n",
    "        samples = self.rng.sample(self.train_audio_paths, k_samples)\n",
    "        feats = [self.featurize(s) for s in samples]\n",
    "        feats = np.vstack(feats)\n",
    "        self.feats_mean = np.mean(feats, axis=0)\n",
    "        self.feats_std = np.std(feats, axis=0)\n",
    "        \n",
    "    def featurize(self, audio_clip):\n",
    "        \"\"\" For a given audio clip, calculate the corresponding feature\n",
    "        Params:\n",
    "            audio_clip (str): Path to the audio clip\n",
    "        \"\"\"\n",
    "        if self.spectrogram:\n",
    "            return spectrogram_from_file(\n",
    "                audio_clip, step=self.step, window=self.window,\n",
    "                max_freq=self.max_freq)\n",
    "        else:\n",
    "            (rate, sig) = wav.read(audio_clip)\n",
    "            return mfcc(sig, rate, numcep=self.mfcc_dim)\n",
    "\n",
    "    def normalize(self, feature, eps=1e-14):\n",
    "        \"\"\" Center a feature using the mean and std\n",
    "        Params:\n",
    "            feature (numpy.ndarray): Feature to normalize\n",
    "        \"\"\"\n",
    "        return (feature - self.feats_mean) / (self.feats_std + eps)\n",
    "\n",
    "    def train_length(self):\n",
    "        return len(self.train_texts)\n",
    "    \n",
    "    def valid_length(self):\n",
    "        return len(self.valid_texts)\n",
    "\n",
    "\n",
    "def shuffle_data(audio_paths, durations, texts):\n",
    "    \"\"\" Shuffle the data (called after making a complete pass through \n",
    "        training or validation data during the training process)\n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.random.permutation(len(audio_paths))\n",
    "    audio_paths = [audio_paths[i] for i in p] \n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def sort_data(audio_paths, durations, texts):\n",
    "    \"\"\" Sort the data by duration \n",
    "    Params:\n",
    "        audio_paths (list): Paths to audio clips\n",
    "        durations (list): Durations of utterances for each audio clip\n",
    "        texts (list): Sentences uttered in each audio clip\n",
    "    \"\"\"\n",
    "    p = np.argsort(durations).tolist()\n",
    "    audio_paths = [audio_paths[i] for i in p]\n",
    "    durations = [durations[i] for i in p] \n",
    "    texts = [texts[i] for i in p]\n",
    "    return audio_paths, durations, texts\n",
    "\n",
    "def vis_train_features(index=0):\n",
    "    \"\"\" Visualizing the data point in the training set at the supplied index\n",
    "    \"\"\"\n",
    "    # obtain spectrogram\n",
    "    audio_gen = AudioGenerator(spectrogram=True)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_audio_path = audio_gen.train_audio_paths[index]\n",
    "    vis_spectrogram_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain mfcc\n",
    "    audio_gen = AudioGenerator(spectrogram=False)\n",
    "    audio_gen.load_train_data()\n",
    "    vis_mfcc_feature = audio_gen.normalize(audio_gen.featurize(vis_audio_path))\n",
    "    # obtain text label\n",
    "    vis_text = audio_gen.train_texts[index]\n",
    "    # obtain raw audio\n",
    "    vis_raw_audio, _ = librosa.load(amharic_path(vis_audio_path))\n",
    "    # print total number of training examples\n",
    "    print('There are %d total training examples.' % len(audio_gen.train_audio_paths))\n",
    "    # return labels for plotting\n",
    "    return vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path\n",
    "\n",
    "\n",
    "def plot_raw_audio(vis_raw_audio, title='Audio Signal', size=(12, 3)):\n",
    "    fig = plt.figure(figsize=size)\n",
    "    ax = fig.add_subplot(111)\n",
    "    steps = len(vis_raw_audio)\n",
    "    ax.plot(np.linspace(1, steps, steps), vis_raw_audio)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "def plot_mfcc_feature(vis_mfcc_feature):\n",
    "    # plot the MFCC feature\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_mfcc_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized MFCC')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('MFCC Coefficient')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    ax.set_xticks(np.arange(0, 13, 2), minor=False);\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram_feature(vis_spectrogram_feature):\n",
    "    # plot the normalized spectrogram\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(vis_spectrogram_feature, cmap=plt.cm.jet, aspect='auto')\n",
    "    plt.title('Normalized Spectrogram')\n",
    "    plt.ylabel('Time')\n",
    "    plt.xlabel('Frequency')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units, activation=activation,\n",
    "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='models/model_1.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ctc_loss(input_to_softmax):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(\n",
    "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "        outputs=loss_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(audio_gen,\n",
    "          input_to_softmax, \n",
    "          model_name,\n",
    "          minibatch_size=20,\n",
    "          optimizer=SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "          epochs=20,\n",
    "          verbose=1):    \n",
    "    # calculate steps_per_epoch\n",
    "    num_train_examples=len(audio_gen.train_audio_paths)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # calculate validation_steps\n",
    "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
    "    validation_steps = num_valid_samples//minibatch_size\n",
    "    \n",
    "    # add CTC loss to the NN specified in input_to_softmax\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "\n",
    "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "\n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "\n",
    "    # add checkpointer\n",
    "    checkpointer = ModelCheckpoint(filepath='models/'+model_name+'.h5', verbose=0)\n",
    "\n",
    "    # train the model\n",
    "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
    "        callbacks=[checkpointer], verbose=verbose, use_multiprocessing=True)\n",
    "\n",
    "    # save model loss\n",
    "    with open('models/'+model_name+'.pickle', 'wb') as f:\n",
    "        pickle.dump(hist.history, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4b2c90d05ee746c0f2a5d752479cf07e3d397ba0ddff19db352cf444be02ed82"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
